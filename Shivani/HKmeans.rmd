
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

loading original data

loading necessary libraries
```{r}

library(factoextra)
library(fpc)
library(RcmdrMisc)
library(sqldf)
```

elbow method to get the optimal k value
```{r}
set.seed(123)
# Elbow method to get best k from k = 2 to k = 12.
k.max <- 12


wss <- sapply(1:k.max, 
              function(k){kmeans(df_scaled, k, nstart=50,iter.max = 15 )$tot.withinss})
wss
plot(1:k.max, wss,
     type="b", pch = 19, frame = FALSE, 
     xlab="Number of clusters K",
     ylab="Total within-clusters sum of squares")
```
HKmeans implementation with k=4
```{r}
#HKmeans implementation
res.hk <-hkmeans(df_scaled, 2)
names(res.hk)

#result summary
res.hk
```
Visualisation and appending the labels obtained after clustering 
```{r}
library(r2d3)
#visualisation
hkmeans_tree(res.hk, cex = 0.2)
fviz_cluster(res.hk, frame.type = "norm", frame.level = 0.68)

#appening the created clusterlabels to data fo further work
df_labeled<-final_data
df_labeled$label<-res.hk$cluster
View(df_labeled)

cluster_features<-data.frame(df_labeled%>%
                               group_by(label)%>%
                               summarise_each(mean))
feature_summary<-data.frame(df%>%
                               summarise_each(mean))

```
Finally creating a data frame which can be used for the visualization.
The difference between "feature_summary" and "cluster_features" will tell us how different the feature of a cluster is from the mean of the population. Which is finally plotted in the graph.
```{r echo=TRUE}
options(scipen = 999)

f<-function(cluster)
 { cluster_plot_data<-rbind(feature_summary,cluster[-1])
  cluster_name<-paste("cluster_",cluster[1])
  rownames(cluster_plot_data)<-c("general_mean",cluster_name)
  cluster_plot_data<-t(cluster_plot_data)%>%data.frame()
  cluster_plot_data<-mutate(cluster_plot_data,mean_difference =   cluster_plot_data[,1]-cluster_plot_data[,2],feature=rownames(cluster_plot_data))
  r2d3(data = cluster_plot_data, script = "cluster_chart.js",viewer ="browser")
}
apply(cluster_features,1,f)

```

supervised phase to obtain the important features using random forest. This chunk does the tuning of the parameters for random forest. There is no inbuilt function to obtain the optimal number of trees i.e mtree could not be tuned using available packages so this code imbibes the parameter tuning for mtree as well and gives the best mtry and ntree parameters. mtry is the number of variables randomly sampled as candidates at each split.
```{r}

df_labeled$cluster <- as.factor(df_labeled$cluster)
library(randomForest)
library(mlbench)
library(caret)
library(e1071)

x <- df_labeled[,1:73]
y <- df_labeled[,74]

#tuning of rf
customRF <- list(type = "Classification", library = "randomForest", loop = NULL)
customRF$parameters <- data.frame(parameter = c("mtry", "ntree"), class = rep("numeric", 2), label = c("mtry", "ntree"))
customRF$grid <- function(x, y, len = NULL, search = "grid") {}
customRF$fit <- function(x, y, wts, param, lev, last, weights, classProbs, ...) {
  randomForest(x, y, mtry = param$mtry, ntree=param$ntree, ...)
}
customRF$predict <- function(modelFit, newdata, preProc = NULL, submodels = NULL)
  predict(modelFit, newdata)
customRF$prob <- function(modelFit, newdata, preProc = NULL, submodels = NULL)
  predict(modelFit, newdata, type = "prob")
# train model
control <- trainControl(method="repeatedcv", number=5, repeats=2)
tunegrid <- expand.grid(.mtry=c(5:12), .ntree=c(500,1000, 1500))
set.seed(123)
custom <- train(cluster~., data=df_labeled, method=customRF,  tuneGrid=tunegrid, trControl=control)
#summary(custom)
plot(custom)
print(custom)
```
applying random forest with the best paramters. for hkmeans number of trees 1000 and mtry 5 is the optimal solution. The features are ordered as per their mean decrease accuracy.
```{r}
data.rf <- randomForest(formula= cluster ~ ., data=df_labeled, importance=TRUE,ntree=1500,mtry=5,
                        proximity=TRUE)

print(data.rf)
plot(data.rf)
varImpPlot(data.rf)  #the first graph shows how worse the model will perfrom after removing each variable and second shows how pure the nodes are at the end of the tree

impfeat<-importance(data.rf)

impfeatdf<-data.frame(impfeat)
impfeatorder<-impfeatdf[order(-impfeatdf$MeanDecreaseAccuracy),]

getorderedfeatures<-function(numberofclusters){
  if(numberofclusters==2)
    impfeatorder$X1<-impfeatorder$X2<-impfeatorder$MeanDecreaseGini<-NULL
  if(numberofclusters==4)
    impfeatorder$X1<-impfeatorder$X2<-impfeatorder$X3<-impfeatorder$X4<-impfeatorder$MeanDecreaseGini<-NULL
  print(impfeatorder)
}
getorderedfeatures(2)


```
```{r}
library(caret)
library(rpart.plot)
library(e1071)
set.seed(123)                                                     #non numeric
trctrl <- trainControl(method = "boot", number = 10)
dtree_fit <- train(cluster ~., data = df_labeled, method = "rpart",
                   parms = list(split = "information"),
                   trControl=trctrl,
                   tuneLength = 10)

prp(dtree_fit$finalModel, box.palette = "Reds", tweak = 1.2)

```

  