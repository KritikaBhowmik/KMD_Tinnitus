---
title: "pre-final"
author: "Abhilash, Kritika, Priyanka, Shivani"
date: "19 August 2019"
output: html_document
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Motivation
* Tinnitus is a highly common health problem which severely affects people. As a Tinnitus patient, people need to fill many
questionnaires which can be tiring.

* If we know the possible sub categories of Tinnitus it would help medical practitioners to provide category specific medical assistance to the patients.

* Understanding the sub categories would also help to identify questionnaires which are only relevant to these sub categories and thus reduce the number questions patients might have to fill.

## Problem Statement
1) To identify relevant sub-categories of Tinnitus condition

2) To represent/describe the identified clusters and assign most relevant questionnaires respectively.

## Organisation

The project is organised as follows:

* Data preprocessing
* Global Clustering
* Subspace Clustering
* Evaluation

***
Let us first load all necessary libraries for this project
```{r Libraries, message=FALSE, warning=FALSE}
#If any errors are thrown while loading libraries please install the packages commented below
#install.packages("tidyverse")
#install.packages("caret")
#install.packages("rpart.plot")
#install.packages("e1071")
library(dplyr)
library(tidyverse)  # data manipulation
library(cluster)    # clustering algorithms
library(factoextra) # clustering visualization
library(dendextend) # for comparing two dendrograms
library(r2d3)
library(sqldf)
library(caret)
library(rpart.plot)
library(e1071)
library(randomForest)
library(mlbench)
library(rJava)
library(subspace)
library(orclus)
library(diceR)
library(clValid)

```

## Data preprocessing 

Now we will step by step do the preprocessing of given data

```{r Dataframe}
df <- read_rds("190426_charite_tinnitus.rds") %>%
  arrange(.testdatum) %>%
  group_by(.jour_nr) %>%
  slice(1) %>%
  ungroup() %>%
  filter(.phase == "A") %>%
  mutate(phqk_paniksyndrom = if_else(phqk_phqk_2a +
                                       phqk_phqk_2b +
                                       phqk_phqk_2c +
                                       phqk_phqk_2d +
                                       phqk_phqk_2e == 5, 1, 0)) %>%
  select(.jour_nr,
         .age,
         acsa_acsa,
         adsl_adsl_sum,
         bi_erschoepfung, bi_magen, bi_glieder, bi_herz, bi_beschwerden,
         bsf_geh, bsf_eng, bsf_aerg, bsf_an_de, bsf_mued, bsf_tnl,
         isr_deprsyn, isr_angstsyn, isr_zwasyn, isr_somasyn, isr_essstsyn,
         isr_zusatz, isr_isr_ges,
         phqk_depressivitaet, phqk_paniksyndrom,
         psq_anford, psq_anspan, psq_freude, psq_sorgen, psq_psq_sum,
         schmerzskal_beein10, schmerzskal_haeuf10, schmerzskal_staerke10,
         ses_ses_affektiv, ses_ses_sensorisch,
         sf8_bp_sf36ks, sf8_gh_sf36ag, sf8_mcs8, sf8_mh_sf36pw, sf8_pcs8,
         sf8_pf_sf36kf, sf8_re_sf36er, sf8_rp_sf36kr, sf8_sf_sf36sf, sf8_vt_sf36vit,
         sozk_soz01_male, sozk_soz02_german, sozk_soz05_partner, sozk_soz06_married,
         sozk_soz09_abitur, sozk_soz10_keinAbschl, sozk_soz11_job, sozk_soz18_selbstst, 
         sozk_soz1920_krank, sozk_soz21_tindauer, sozk_soz2224_psycho, sozk_soz25_numdoc,
         swop_sw, swop_opt, swop_pes,
         tq_aku, tq_co, tq_em, tq_inti, tq_pb, tq_sl, tq_som, tq_tf,
         tinskal_beein10, tinskal_haeuf10, tinskal_laut10,
         starts_with("tlq"), -tlq_timestamp
  ) %>%
  drop_na()
originaldf <- read_rds("190426_charite_tinnitus.rds")
```
Now lets remove 'journ.no' as it is an identifier and will not be useful for analysis.

```{r Dataframe all features}
df_allF <- select(df,-c(.jour_nr))

#Data frame with all features "Scaled" except journ no
df_allF_scaled<-scale(df_allF)%>%data.frame()
```

We will find out the co related columns in the data and drop them

```{r Correlation}
correlated_coloumns <- data.frame(F1 = character(),F2 = character(),coef = numeric())

#cat("\ncorrelation with 90%:\n")
matriz_cor <- cor(df_allF,method = "spearman")

for (i in 1:nrow(matriz_cor)){
  correlations <-  which((abs(matriz_cor[i,]) > 0.9) & (matriz_cor[i,] != 1))
  matriz_cor[correlations,i] <- NA
  
  if(length(correlations)> 0){
    #lapply(correlations,FUN =  function(x) (cat("\t",paste(colnames(test)[i], "with",colnames(test)[x]), "\n")))
    correlated_coloumns <-  rbind(correlated_coloumns,data.frame(F1=colnames(df_allF)[i],F2=colnames(df_allF)[correlations],coef=matriz_cor[i,correlations]))
    rownames(correlated_coloumns) <- NULL
  }
}

#No correlated columns

#dropping the columns
df_noCorr <- select(df_allF,-c("sf8_mh_sf36pw","tq_tf","tq_em","tq_co"))

#Data frame with reduced features "Scaled"
df_noCorr_scaled <- scale(df_noCorr)%>%data.frame()
```
## Elbow Method
Elbow method for getting optimal value of k. The idea behind elbow method is to to define number of clusters such that the total within-cluster sum of square (WSS) is minimized. The value of chosen k is such that adding another cluster does not affect the overall wss much.
```{r Elbow Method}
set.seed(123)
# Elbow method to get best k from k = 2 to k = 12.
k.max <- 12
rownames( df_noCorr_scaled ) <- seq_len( nrow( df_noCorr_scaled ) )
wss <- sapply(1:k.max, 
              function(k){kmeans(df_noCorr_scaled, k, nstart=50,iter.max = 15 )$tot.withinss})
wss
plot(1:k.max, wss,
     type="b", pch = 19, frame = FALSE, 
     xlab="Number of clusters K",
     ylab="Total within-clusters sum of squares")
```


## Bootstrap Sampling

Create different samples (with replacement) of data from the original dataset. With varying data (different samples), we will can check if the clustering results are stable or not. So we are plotting a Box-plot for each values of K against "Total within sum of square errors (totwss)". We need to check if the result follows the same trend as the original elbow method does, or there is a significant change in the plot. 

```{r Bootstrap}
# #No. of Bootsamples to be created
# bootsamplesize <- seq(1:100)
# 
# max_k <-seq(1:12) 
# wss.boot <- data.frame()
# 
# set.seed(123)
# for(i in bootsamplesize){
# 
#   #Create Index with replacement
#   index <- sample(1:nrow(df_noCorr_scaled),nrow(df_noCorr_scaled),replace = T)
#   
#   #create samples from the index
#   dfSample <-  df_noCorr_scaled[index,]
#   
#   #run for each value of k for the current sample  
#   for(k in max_k){
#       
#     kmclust <- kmeans(dfSample,k,nstart=50,iter.max = 25)
#     
#     #Store the data (1 row) of current K and withinss in the dataframe  
#     wss.boot <- rbind(wss.boot, c(k,kmclust$tot.withinss))
#     
#   }
# }
# 
# colnames(wss.boot) <- c('k','totwss')
# 
# 
# #Plot the Data
# boxplot(totwss~k, data = wss.boot, xlab = "k values", ylab = "totwss", main = "Bootsampled Data: Kmeans Cluster withinss")
# 

```


## Hierarchical clustering

Hierarchical clustering can be done in two ways :
1) Top Down : Start with the entire data as a cluster and break them down until each point is a cluster in itself.
2) Bottom up : Start from individual points as clusters and merge them based on similarities until one cluster if formed.

There are different ways to calculate the distance/ similarity between individual points and in between the clusters which can be a very important factor in deciding the final cluster results.

So we first create a list of these methods and then create clusters for each combination.

Then we can calculate the clusters. The ac value of each cluster gives the quality/ tightness of each cluster. The closer the value is to 1 the better are the results.

**the agglomerative coefficient, measuring the clustering structure of the dataset.

For each observation i, denote by m(i) its dissimilarity to the first cluster it is merged with, divided by the dissimilarity of the merger in the final step of the algorithm. The ac is the average of all 1 - m(i). It can also be seen as the average width (or the percentage filled) of the banner plot. Because ac grows with the number of observations, this measure should not be used to compare datasets of very different sizes.**
```{r Hierarchical Tune, echo=FALSE}
set.seed(123)
# methods to assess inter cluster similarity
m <- c( "average", "single", "complete", "ward")
names(m) <- c( "average", "single", "complete", "ward")

#distance measures
dist_methods <- c("euclidean","manhattan","maximum", "canberra","minkowski")
names(dist_methods)<- c("euclidean","manhattan","maximum", "canberra","minkowski")

# function to compute coefficient
final_dist_m<-list()
final_m<-list()
for(dist_m in dist_methods)
{
  dist_matrix<-dist(df_noCorr_scaled, method = dist_m)
  for(x in m)
  {
    hc <-agnes(dist_matrix, method = x)
    ac<-hc$ac
    title<-sprintf("Distance: %s,Inter cluster Distance: %s, ac value: %f",dist_m,x,ac)
    pltree(hc, cex = 0.6, hang = -1,main = title)
    if(!is.nan(ac) && ac>0.95)
    {
      print(title)
      final_dist_m<-c(final_dist_m,dist_m)
      final_m<-c(final_m,x)
    }
  }
}
```


```{r Hierarchical}
#Now we can choose the desired combination, for example: Euclidean and Ward method and then we can cut the tree to get the final clusters.

dist_matrix<-dist(df_noCorr_scaled, method = "euclidean")
hc <-agnes(dist_matrix, method = "ward")
ac<-hc$ac
    title<-sprintf("Distance: %s,Inter cluster Distance: %s, ac value: %f","eucledian","ward",ac)
    pltree(hc, cex = 0.6, hang = -1,main = title)
```

## PCA-Kmeans
Prinicipal Component Analysis is used for High Dimensional Dataset to convert the possibly correlated features into linearly uncorrelated variables called Principal Components (PCs). The first Principal Component (PC) explains the most amount of variance in the data. The PCs are arranged in the decreasing order of variance explained. This can be seen in Scree Plots. To explain certain amount of variance of the data, we need to choose the subsequent amount of the PC number. This can be done using the help of the Score Plots. After selecting first n PCs, we need to save that in another dataframe and then we can perform our clustering algorithm on the new transformed data.


```{r PCA}
#Create Prinicipal Components
km.pc <- prcomp(df_noCorr_scaled, center = TRUE)
#summary(km.pc)

#Maxmimum PC for Scree and Score Plots for visualization
maxPC <- 50

screeplot(km.pc, type = "l", npcs = maxPC, main = paste("Screeplot of the first ", maxPC," PCs"))

#eigenvalues <1 would mean that the component actually explains less than a single explanatory variable 
abline(h = 1, col="red", lty=5)
legend("topright", legend=c("Eigenvalue = 1"),
       col=c("red"), lty=5, cex=0.6)


#No.of PCs required to explain variance
PC_num <- 18
PC_var <- 0.70

cumpro <- cumsum(km.pc$sdev^2 / sum(km.pc$sdev^2))
plot(cumpro[0:maxPC], xlab = "PC Numbers", ylab = "Amount of explained variance", main = paste("Score plot of variance =", PC_var*100, "% at PC =",PC_num))


abline(v = PC_num, col="blue", lty=5)
abline(h = PC_var, col="blue", lty=5)

legend("topleft", legend=c(paste("Cut-off @ PC",PC_num)), col=c("blue"), lty=5, cex=0.6)


#Creating dataset with first PC_num PCs that we need to explain variance
df_PC <- km.pc$x[,1:PC_num]


```

## ORCLUS 
Many datasets contains inter-attribute correlations. Orclus is used to tackle this by extending Proclus to include search for non-axis parallel subspaces. Orclus is based on K-means and has three phases: Assign clusters, Find Vectors and Merge. Initially the data points are assigned to the nearest cluster centers with minimal euclidean distance in corresponding subspaces. In the next phase, for each cluster, subspace dimensionality $l_c$ are determined which is closer to the user input l. From iteration to iteration $l_c$ values decreases gradually to match $l$. During the Merge phase closest current pairs of clusters are merged to get final k clusters from initial k0 clusters. 

```{r Orclus,echo=FALSE}

#Represents the tuning of clusters according to quality measures

 tune_orclus <- function(data,num_of_clusters)
{
  for (k in seq(10,30,by = 5)) {
  
    for (i in seq(20,60,by = 5)) {
  
      orclus_res_k2 <- orclus(x = data,k=2,l = i, k0 = k)
      tune_lf_sparsity[j,1] <- k
      tune_lf_sparsity[j,2] <- i
      tune_lf_sparsity[j,3] <- orclus_res_k2$sparsity.coefficient
      tune_lf_sparsity[j,4] <- sum(orclus_res_k2$within.projens)
      j <- j+1
    }
  }
}

#tune_orclus(df_noCorr_scaled,2)
#tune_orclus(df_noCorr_scaled,4)

```
## PROCLUS

Proclus is based on K-mediods and uses three phase approach: initialization, iteration and cluster refinement. In the initialization step it selects random set of mediods which are farthest from each other. It uses Sampling to get data set and set of K mediods. These mediods are then in next phase iteratively selected to improve the clustering.  This is done by choosing average distance between points and the nearest mediod as the Cluster Quality measure. In refinement phase, based on the cluster formed new dimensions for each mediods are computed. Proclus can produce cluster with different number of selected dimensions as it tries to maintain the average dimensions per cluster given by the user.

```{r Proclus}

#For K = 2
ProClus_clusters_k2<- ProClus(df_noCorr_scaled,k=2,d=20)
#Getting each cluster elements
k2_cluster1<-ProClus_clusters_k2[[1]]$objects
k2_cluster2<-ProClus_clusters_k2[[2]]$objects

# For K = 4
ProClus_clusters_k4<- ProClus(df_noCorr_scaled,k=4,d=20)
#Getting each cluster elements
k4_cluster1<-ProClus_clusters_k4[[1]]$objects
k4_cluster2<-ProClus_clusters_k4[[2]]$objects
k4_cluster3<-ProClus_clusters_k4[[3]]$objects
k4_cluster4<-ProClus_clusters_k4[[4]]$objects
```


## Visualisation

Creating a summary data frame for capturing population mean
```{r Feature Summary}

feature_summary_scaled<-data.frame(df_allF_scaled%>%
                               summarise_each(mean))
feature_summary<-data.frame(df_allF%>%
                              summarise_each(mean))
```
For visualisation of clustering results we choose a radial chart.
To generate this radial chart we need format the data in a data structure suitable for plotting.
The bar height is the scaled value of the features of a cluster. The tooltip gives extra information about the actual feature value and the population mean for the feature.
```{r Visualisation}
#Finally creating a data frame which can be used for the visualization.
options(scipen = 999)
createPlotData<-function(algo_name,clus_name,cluster_scaled,cluster)
 { cluster_plot_data<-rbind(colnames(cluster[-1]),cluster_scaled[-1],feature_summary,cluster[-1])
  rownames(cluster_plot_data)<-c("feature","scaled_cluster_feature_value","population_mean","cluster_feature_value")
  cluster_plot_data<-t(cluster_plot_data)%>%data.frame()%>%mutate(algorithm=algo_name,cluster_name=clus_name)
  return(cluster_plot_data)
}


getPlotData<-function(algo_name,final_labels,no_of_clusters)
{
  # labelling the records 
  df_allF_labeled<-df_allF%>%
  mutate(label = final_labels)
df_allF_scaled_labeled<-df_allF_scaled%>%
  mutate(label = final_labels)

# creating cluster feature vectors from obtained labels

cluster_features_scaled<-data.frame(df_allF_scaled_labeled%>%
                               group_by(label)%>%
                               summarise_each(mean))
cluster_features<-data.frame(df_allF_labeled%>%
                               group_by(label)%>%
                               summarise_each(mean))
  plotData<-list()
  for(i in seq(1:no_of_clusters))
  {
    clus_name<-paste("Type ",i)
    plotData[[i]]<-createPlotData(algo_name,clus_name,cluster_features_scaled[i,],cluster_features[i,])
  }
  return(plotData)
}

getProclusPlotData<- function(algo_name,proclus_cluster,indices,cluster_num)
{
 # labelling the records 
  df_noCorr_labeled<-df_noCorr[indices,which(proclus_cluster[[cluster_num]][["subspace"]] == TRUE)]%>%
  mutate(label = cluster_num)
df_noCorr_scaled_labeled<-df_noCorr_scaled[indices,which(proclus_cluster[[cluster_num]][["subspace"]] == TRUE)]%>%
  mutate(label = cluster_num)

# creating cluster feature vectors from obtained labels

cluster_feature_scaled<-data.frame(df_noCorr_scaled_labeled%>%
                               group_by(label)%>%
                               summarise_each(mean))
cluster_feature<-data.frame(df_noCorr_labeled%>%
                               group_by(label)%>%
                               summarise_each(mean))
p_feature_summary<-data.frame(df_noCorr%>%
                              summarise_each(mean))
p_feature_summary<-p_feature_summary[,which(proclus_cluster[[cluster_num]][["subspace"]] == TRUE)]
  plotData<-list()
  clus_name<-paste("Type ",cluster_num)
  cluster_plot_data<-rbind(colnames(cluster_feature[-1]),cluster_feature_scaled[-1],p_feature_summary,cluster_feature[-1])
  rownames(cluster_plot_data)<-c("feature","scaled_cluster_feature_value","population_mean","cluster_feature_value")
  cluster_plot_data<-t(cluster_plot_data)%>%data.frame()%>%mutate(algorithm=algo_name,cluster_name=clus_name)
  return(cluster_plot_data)
}
```
## Decision Tree

Now we have plotted all the feature values for each cluster. However for each cluster there are only a few features which clearly distinguish it from other clusters. To find out which features and their respective values are responsible to identify a cluster we can use supervised techniques.
Let us now use a Decision Tree algorithm to identify which features are the most important to lead us to get our cluster labels as leaf nodes.

Now to get the decision tree as a classifier intead of a regressor we need to make the predicting variable as non numeric.
```{r DT}
createDT<-function(final_labels,remove_na)
{ 
  set.seed(123)
  
  # labelling the records 
  df_allF_labeled<-df_allF%>%
  mutate(label = final_labels)
  df_string_labeled<-df_allF_labeled
  
  #Adding "Type" to each cluster number to make it non numeric
  df_string_labeled$label<-sub("^","Type ",df_allF_labeled$label)
  if(remove_na)
  {
    df_string_labeled <- df_string_labeled[-c(which(is.na(df_string_labeled$label))),]
  }
  
  trctrl <- trainControl(method = "boot", number = 10)
  dtree_fit <- train(label ~., data = df_string_labeled, method = "rpart",
                     parms = list(split = "information"),
                     trControl=trctrl,
                     tuneLength = 10)
  
  prp(dtree_fit$finalModel,extra = 2, box.palette = "Reds", tweak = 1.5,varlen
=-10, branch.type = 5)
  #rpart.plot::rpart.plot(dtree_fit$finalModel, branch.type = 5, digits = 5, tweak = 1.9, fallen.leaves = FALSE)
}

```


## Random Forest

Supervised phase to obtain the important features using random forest. This chunk does the tuning of the parameters for random forest. There is no inbuilt function to obtain the optimal number of trees i.e mtree could not be tuned using available packages so this code does the parameter tuning for mtree as well and gives the best mtry and ntree parameters. mtry is the number of variables randomly sampled as candidates at each split.
We then apply random forest with the best paramters. 
##{todo}table with optimal values after parameter tuning.
The features are then ordered as per their mean decrease accuracy.
```{r RF}
set.seed(123)
##Tune random forest
tuneRandomForest<-function(labels){
# labelling the records 
data<-df_allF%>%
  mutate(label = labels)
data$label<- as.factor(data$label)
x <- data[,1:77]
y <- data[,78]
#tuning of rf
customRF <- list(type = "Classification", library = "randomForest", loop = NULL)
customRF$parameters <- data.frame(parameter = c("mtry", "ntree"), class = rep("numeric", 2), label = c("mtry", "ntree"))
customRF$grid <- function(x, y, len = NULL, search = "grid") {}
customRF$fit <- function(x, y, wts, param, lev, last, weights, classProbs, ...) {
  randomForest(x, y, mtry = param$mtry, ntree=param$ntree, ...)
}
customRF$predict <- function(modelFit, newdata, preProc = NULL, submodels = NULL)
  predict(modelFit, newdata)
customRF$prob <- function(modelFit, newdata, preProc = NULL, submodels = NULL)
  predict(modelFit, newdata, type = "prob")
# train model
control <- trainControl(method="repeatedcv", number=5, repeats=2)
tunegrid <- expand.grid(.mtry=c(5:12), .ntree=c(500,1000, 1500))
set.seed(123)
custom <- train(label~., data=data, method=customRF,  tuneGrid=tunegrid, trControl=control)
#summary(custom)
plot(custom)
print(custom)
}

##applying random forest
applyrf<-function(labels,numClusters,ntree,mtry){
# labelling the records 
data<-df_allF%>%
  mutate(label = labels)
data$label<- as.factor(data$label)
data.rf <- randomForest(formula= label ~ ., data=data, importance=TRUE,ntree=ntree,mtry=mtry,
                        proximity=TRUE)
print(data.rf)
plot(data.rf)
varImpPlot(data.rf)  #the first graph shows how worse the model will perfrom after removing each variable and second shows how pure the nodes are at the end of the tree
impfeat<-importance(data.rf)
impfeatdf<-data.frame(impfeat)
impfeatorder<-impfeatdf[order(-impfeatdf$MeanDecreaseAccuracy),]
if(numClusters==2)
  impfeatorder$X1<-impfeatorder$X2<-impfeatorder$MeanDecreaseGini<-NULL
if(numClusters==4)
  impfeatorder$X1<-impfeatorder$X2<-impfeatorder$X3<-impfeatorder$X4<-impfeatorder$MeanDecreaseGini<-NULL
print(impfeatorder)
}
```

## Note: 
To be run after creating labels below. Used only once to Tune the model parameters for building Random Forest.

```{r RF Tune}
##function call
# tuneRandomForest(hk2_labels)
# tuneRandomForest(km2_labels)
# tuneRandomForest(h2_labels)
# tuneRandomForest(pckm2_labels)
# tuneRandomForest(ok2_labels)
# tuneRandomForest(p2_labels)
# tuneRandomForest(hk4_labels)
# tuneRandomForest(km4_labels)
# tuneRandomForest(h4_labels)
# tuneRandomForest(pckm4_labels)
# tuneRandomForest(p4_labels)
# tuneRandomForest(ok4_labels)

```


## Performing Radial Chart Visualisation and Decision Tree for selected Cluster numbers
First the Labels are fetched for each clustering algorithm, which is then passed into the initVis function alongwith the cluster number to create Radial Chart visualisations. Decision tree function is then called to visualize the decision points that can be used by the user to ask the relevant questions to come to a sub-group of Tinnitus.

# Fetching labels for k=2 

```{r Model Run k2}
set.seed(123)
removeNA <- 0 #Flag
num_clusters <- 2

# k means 
set.seed(123)
kmclust2 <- kmeans(df_noCorr_scaled,num_clusters, nstart=50,iter.max = 15)
km2_labels <- kmclust2$cluster 
cluster_plot_data<-getPlotData("K-means",km2_labels,num_clusters)
r2d3(data = cluster_plot_data[[1]], script = "cluster_chart.js",viewer ="internal")
r2d3(data = cluster_plot_data[[2]], script = "cluster_chart.js",viewer ="internal")
createDT(km2_labels,removeNA)
applyrf(km2_labels,2,1000,5)

# Hierarchical
set.seed(123)
h2_labels<-cutree(hc,k=num_clusters)
cluster_plot_data<-getPlotData("Hierarchical Clustering",h2_labels,num_clusters)
r2d3(data = cluster_plot_data[[1]], script = "cluster_chart.js",viewer ="internal")
r2d3(data = cluster_plot_data[[2]], script = "cluster_chart.js",viewer ="internal")
createDT(h2_labels,removeNA)
applyrf(h2_labels,2,1500,6)

#PCA- KMEANS
set.seed(123)
pckmclust2 <- kmeans(df_PC,num_clusters, nstart=50,iter.max = 15)
pckm2_labels <- pckmclust2$cluster
cluster_plot_data<-getPlotData("PCA K-means",pckm2_labels,num_clusters)
r2d3(data = cluster_plot_data[[1]], script = "cluster_chart.js",viewer ="internal")
r2d3(data = cluster_plot_data[[2]], script = "cluster_chart.js",viewer ="internal")
createDT(pckm2_labels,removeNA)
applyrf(pckm2_labels,2,1000,5)

# HK means
set.seed(123)
res.hk2 <-hkmeans(df_noCorr_scaled, num_clusters)
hk2_labels <- res.hk2$cluster
cluster_plot_data<-getPlotData("HK-means",hk2_labels,num_clusters)
r2d3(data = cluster_plot_data[[1]], script = "cluster_chart.js",viewer ="internal")
r2d3(data = cluster_plot_data[[2]], script = "cluster_chart.js",viewer ="internal")
createDT(hk2_labels,removeNA)
applyrf(hk2_labels,2,1500,5)

# Orclus
set.seed(123)
orclus_res_k2 <- orclus(x = df_noCorr_scaled,k=num_clusters,l = 25, k0 = 20)
ok2_labels <- orclus_res_k2$cluster
cluster_plot_data<-getPlotData("Orclus",ok2_labels,num_clusters)
r2d3(data = cluster_plot_data[[1]], script = "cluster_chart.js",viewer ="internal")
r2d3(data = cluster_plot_data[[2]], script = "cluster_chart.js",viewer ="internal")
createDT(ok2_labels,removeNA)
applyrf(ok2_labels,2,1500,10)

# Proclus
set.seed(123)
removeNA <- 1
p2_labels<-c()
for(i in k2_cluster1){
  p2_labels[i] <- 1
}
for(i in k2_cluster2){

  p2_labels[i]<- 2
}
#cluster_plot_data<-getPlotData("Proclus",p2_labels,num_clusters)

r2d3(data = getProclusPlotData("Proclus",ProClus_clusters_k2,k2_cluster1,1), script = "cluster_chart.js",viewer ="internal")
r2d3(data = getProclusPlotData("Proclus",ProClus_clusters_k2,k2_cluster2,2), script = "cluster_chart.js",viewer ="internal")
createDT(p2_labels,removeNA)
#applyrf(p2_labels,2,1000,5) 

```

# Fetching labels for k=4
```{r Model Run k4}
set.seed(123)
removeNA<- 0
num_clusters<-4

# k means
set.seed(123)
kmclust4 <- kmeans(df_noCorr_scaled,num_clusters, nstart=50,iter.max = 15)
km4_labels <- kmclust4$cluster
cluster_plot_data<-getPlotData("K-means",km4_labels,num_clusters)
r2d3(data = cluster_plot_data[[1]], script = "cluster_chart.js",viewer ="internal")
r2d3(data = cluster_plot_data[[2]], script = "cluster_chart.js",viewer ="internal")
r2d3(data = cluster_plot_data[[3]], script = "cluster_chart.js",viewer ="internal")
r2d3(data = cluster_plot_data[[4]], script = "cluster_chart.js",viewer ="internal")
createDT(km4_labels,removeNA)
applyrf(km4_labels,2,1000,5)

# Hierarchical
set.seed(123)
h4_labels<-cutree(hc,k=num_clusters)
cluster_plot_data<-getPlotData("Hierarchical Clustering",h4_labels,num_clusters)
r2d3(data = cluster_plot_data[[1]], script = "cluster_chart.js",viewer ="internal")
r2d3(data = cluster_plot_data[[2]], script = "cluster_chart.js",viewer ="internal")
r2d3(data = cluster_plot_data[[3]], script = "cluster_chart.js",viewer ="internal")
r2d3(data = cluster_plot_data[[4]], script = "cluster_chart.js",viewer ="internal")
createDT(h4_labels,removeNA)
applyrf(h4_labels,2,1500,6)

#PCA- KMEANS
set.seed(123)
pckmclust4 <- kmeans(df_PC,num_clusters, nstart=50,iter.max = 15)
pckm4_labels <- pckmclust4$cluster
cluster_plot_data<-getPlotData("PCA K-means",pckm4_labels,num_clusters)
r2d3(data = cluster_plot_data[[1]], script = "cluster_chart.js",viewer ="internal")
r2d3(data = cluster_plot_data[[2]], script = "cluster_chart.js",viewer ="internal")
r2d3(data = cluster_plot_data[[3]], script = "cluster_chart.js",viewer ="internal")
r2d3(data = cluster_plot_data[[4]], script = "cluster_chart.js",viewer ="internal")
createDT(pckm4_labels,removeNA)
applyrf(pckm2_labels,2,1000,5)

#HK Means
set.seed(123)
res.hk4 <-hkmeans(df_noCorr_scaled, num_clusters)
hk4_labels <- res.hk4$cluster
cluster_plot_data<-getPlotData("Hierarchical K-means",hk4_labels,num_clusters)
r2d3(data = cluster_plot_data[[1]], script = "cluster_chart.js",viewer ="internal")
r2d3(data = cluster_plot_data[[2]], script = "cluster_chart.js",viewer ="internal")
r2d3(data = cluster_plot_data[[3]], script = "cluster_chart.js",viewer ="internal")
r2d3(data = cluster_plot_data[[4]], script = "cluster_chart.js",viewer ="internal")
createDT(hk4_labels,removeNA)
applyrf(hk4_labels,4,1500,5)

#Orclus
set.seed(123)
orclus_res_k4 <- orclus(x = df_noCorr_scaled,k=num_clusters,l = 20, k0 = 31)
ok4_labels <- orclus_res_k4$cluster
cluster_plot_data<-getPlotData("Orclus",ok4_labels,num_clusters)
r2d3(data = cluster_plot_data[[1]], script = "cluster_chart.js",viewer ="internal")
r2d3(data = cluster_plot_data[[2]], script = "cluster_chart.js",viewer ="internal")
r2d3(data = cluster_plot_data[[3]], script = "cluster_chart.js",viewer ="internal")
r2d3(data = cluster_plot_data[[4]], script = "cluster_chart.js",viewer ="internal")
createDT(ok4_labels,removeNA)
applyrf(ok4_labels,2,1500,12)

# Proclus
set.seed(123)
removeNA <- 1
p4_labels<-c()
for(i in k4_cluster1){
  p4_labels[i] <- 1
}
for(i in k4_cluster2){

  p4_labels[i]<- 2
}
for(i in k4_cluster3){
  p4_labels[i] <- 3
}
for(i in k4_cluster4){

  p4_labels[i]<- 4
}
#cluster_plot_data<-getPlotData("Proclus",p4_labels,num_clusters)
r2d3(data = getProclusPlotData("Proclus",ProClus_clusters_k4,k4_cluster1,1), script = "cluster_chart.js",viewer ="internal")
r2d3(data = getProclusPlotData("Proclus",ProClus_clusters_k4,k4_cluster2,2), script = "cluster_chart.js",viewer ="internal")
r2d3(data = getProclusPlotData("Proclus",ProClus_clusters_k4,k4_cluster3,3), script = "cluster_chart.js",viewer ="internal")
r2d3(data = getProclusPlotData("Proclus",ProClus_clusters_k4,k4_cluster4,4), script = "cluster_chart.js",viewer ="internal")
createDT(p4_labels,removeNA)
#applyrf(p4_labels,2,1000,12)

```
## Evaluation :
* Dunn Index: It is the ratio of minimum of this pairwise distance as the inter-cluster separation (min.separation) to the maximal intra-cluster distance (i.e maximum diameter) as the intra-cluster compactness.If the data set contains compact and well-separated clusters, the diameter of the clusters is expected to be small and the distance between the clusters is expected to be large. Thus, Dunn index should be maximized.
* Connectivity: corresponds to what extent items are placed in the same cluster as their nearest neighbors in the data space. The connectivity has a value between 0 and infinity and should be minimized.
* Compactness or cluster cohesion: Measures how close are the objects within the same cluster. A lower within-cluster variation is an indicator of a good compactness (i.e.a good clustering). Smaller values indicate a better clustering structure.
```{r Evaluation, echo=FALSE}
evaluate<-function(labels){
  
  result_set<-vector("list", 3)
  result_set[1]<-dunn(clusters = labels, Data=df_allF_scaled)
  result_set[2]<-connectivity(distance = NULL, labels, Data = df_allF_scaled, neighbSize = 10, method = "euclidean")
  result_set[3]<-compactness(df_allF_scaled, labels)
  print(result_set)
  return(result_set)
}

kmeans_result_2<-evaluate(km2_labels)
hkmeans_result_2<-evaluate(hk2_labels)
hc_result_2<-evaluate(h2_labels)
pcakmeans_result_2<-evaluate(pckm2_labels)

kmeans_result_4<-evaluate(km4_labels)
hkmeans_result_4<-evaluate(hk4_labels)
hc_result_4<-evaluate(h4_labels)
pcakmeans_result_4<-evaluate(pckm4_labels)
```

Following Table is created to store the evaluation measure values for the used Clustering approaches. 

```{r Evaluation Table}
eval_table <- matrix(, nrow = 1, ncol = 3)
eval_table<-rbind(eval_table,kmeans_result_2)
eval_table<-rbind(eval_table,hkmeans_result_2)
eval_table<-rbind(eval_table,hc_result_2)
eval_table<-rbind(eval_table,pcakmeans_result_2)
colnames(eval_table)<-c("Dunn","Connectivity","Compactness")
eval_table<-rbind(eval_table,kmeans_result_4)
eval_table<-rbind(eval_table,hkmeans_result_4)
eval_table<-rbind(eval_table,hc_result_4)
eval_table<-rbind(eval_table,pcakmeans_result_4)
eval_table<-eval_table[-1,]

print(eval_table)
```