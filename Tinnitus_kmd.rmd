---
title: "Tinnitus Subtypes Identification with Clustering and Result Interpretation using Radial Bar Chart and Surrogate Models"
author: "Abhilash, Kritika, Priyanka, Shivani"
date: "6 September 2019"
output: html_document
---

## Motivation

* Tinnitus is a highly common health problem which severely affects people. As a Tinnitus patient, people need to fill many
questionnaires which can be tiring.

* If we know the possible sub categories of Tinnitus it would help medical practitioners to provide category specific medical assistance to the patients.

* Understanding the sub categories would also help to identify questionnaires which are only relevant to these sub categories and thus reduce the number questions patients might have to fill.

## Problem Statement

1) To identify relevant sub-categories of Tinnitus condition

2) To represent/describe the identified clusters and assign most relevant questionnaires respectively.

## Organisation

The project is organised as follows:

* Data preprocessing
* Clustering
* Visualisation
* Evaluation

```{r setup, include=TRUE, message=FALSE,warning=FALSE}
knitr::opts_chunk$set(echo = TRUE)
remove(list=ls())
gc(reset = TRUE)
#User Configuration

#-----------------Data Pre-processing------------------------------------------------------#
dataFilePath<-"190426_charite_tinnitus.rds"
correlationValue<-0.9  #choose from 0.1 to 1.0



#----------------Cluster Configurations----------------------------------------------------#

####################  choose number of clusters  from 2 to 10  ##################

#-----------------K-means------------------------------------------------------------------#

numClustKmeans<-2

#-----------------Hierarchical-------------------------------------------------------------#

numClusthierarchical<-2
#clustering method("ward","average","single","complete","weighted")
clustering_method_hierarchical<-"ward"
#do you want to tune hierarchical clustering to get the best method
tuneHierarchical<-FALSE
numClusthKmeans<-2
clustering_method_hkmeans<-"ward"

#-----------------Proclus-----------------------------------------------------------------#

numClustProclus<-4
#Do you want to reproduce the proclus results that were described in the report(True/False)
reproduceProclusResults<-FALSE
######### path where the proclus result object is stored #########
path<-"data 2 .Rdata"
avgDimProclus<-25

#-----------------Orclus------------------------------------------------------------------#

numClustOrclus<-2
finalSubspaceDimOrclus<-20
initialNumberOfClustersOrclus<-25
#do you want to tune Orclus
tuneOrclus<-TRUE

#-----------------PCA K-means-------------------------------------------------------------#

numClustPCAKmeans<-2
variancePCAKmeans<-0.7 #choose from 0.1 to 1.0

#-----------------DT Validation-----------------------------------------------------------#

######## Do you want to validate the obtained DT using Random Forest #######
valDTkmeans<-TRUE
valDThkmeans<-TRUE
valDThierarchical<-TRUE
valDTProclus<-TRUE
valDTOrclus<-TRUE
valDTPCAKmeans<-TRUE


```

Let us first load all necessary libraries for this project
```{r Libraries, message=FALSE, warning=FALSE}
library(dplyr)
library(tidyverse)  
library(cluster)    
library(dendextend)
library(r2d3)
library(caret)
library(rpart.plot)
library(e1071)
library(mlbench)
library(rJava)
library(clValid)
library(randomForest)

```

## Data preprocessing 

Now we will step by step do the preprocessing of given data.

```{r Dataframe}
df <- read_rds(dataFilePath) %>%
  arrange(.testdatum) %>%
  group_by(.jour_nr) %>%
  slice(1) %>%
  ungroup() %>%
  filter(.phase == "A") %>%
  mutate(phqk_paniksyndrom = if_else(phqk_phqk_2a +
                                       phqk_phqk_2b +
                                       phqk_phqk_2c +
                                       phqk_phqk_2d +
                                       phqk_phqk_2e == 5, 1, 0)) %>%
  select(.jour_nr,
         .age,
         acsa_acsa,
         adsl_adsl_sum,
         bi_erschoepfung, bi_magen, bi_glieder, bi_herz, bi_beschwerden,
         bsf_geh, bsf_eng, bsf_aerg, bsf_an_de, bsf_mued, bsf_tnl,
         isr_deprsyn, isr_angstsyn, isr_zwasyn, isr_somasyn, isr_essstsyn,
         isr_zusatz, isr_isr_ges,
         phqk_depressivitaet, phqk_paniksyndrom,
         psq_anford, psq_anspan, psq_freude, psq_sorgen, psq_psq_sum,
         schmerzskal_beein10, schmerzskal_haeuf10, schmerzskal_staerke10,
         ses_ses_affektiv, ses_ses_sensorisch,
         sf8_bp_sf36ks, sf8_gh_sf36ag, sf8_mcs8, sf8_mh_sf36pw, sf8_pcs8,
         sf8_pf_sf36kf, sf8_re_sf36er, sf8_rp_sf36kr, sf8_sf_sf36sf, sf8_vt_sf36vit,
         sozk_soz01_male, sozk_soz02_german, sozk_soz05_partner, sozk_soz06_married,
         sozk_soz09_abitur, sozk_soz10_keinAbschl, sozk_soz11_job, sozk_soz18_selbstst, 
         sozk_soz1920_krank, sozk_soz21_tindauer, sozk_soz2224_psycho, sozk_soz25_numdoc,
         swop_sw, swop_opt, swop_pes,
         tq_aku, tq_co, tq_em, tq_inti, tq_pb, tq_sl, tq_som, tq_tf,
         tinskal_beein10, tinskal_haeuf10, tinskal_laut10,
         starts_with("tlq"), -tlq_timestamp
  ) %>%
  drop_na()
originaldf <- read_rds(dataFilePath)
```

Now lets remove the identifier as it will not be useful for analysis.

```{r Dataframe all features}
df_allF <- select(df,-c(.jour_nr))

#Data frame with all features Scaled except ".journ_no"
df_allF_scaled<-scale(df_allF)%>%data.frame()
```

We will find out the correlated columns in the data and drop them.

```{r Correlation}
correlations <- function(cor_threshold)
{
  correlated_coloumns <- data.frame(F1 = character(),F2 = character())
 
  matriz_cor <- cor(df_allF,method = "spearman")
  
  for (i in 1:nrow(matriz_cor)){
    correlations <-  which((abs(matriz_cor[i,]) > cor_threshold) & (matriz_cor[i,] != 1))
    matriz_cor[correlations,i] <- NA
    
    if(length(correlations)> 0){
      
      correlated_coloumns <-  rbind(correlated_coloumns,data.frame(F1=colnames(df_allF)[i],F2=colnames(df_allF)[correlations]))
      rownames(correlated_coloumns) <- NULL
    }
  }
  
  
  x <- as.list(sqldf::sqldf("SELECT distinct(F1) as feat FROM correlated_coloumns UNION SELECT distinct(F2) FROM correlated_coloumns") )
  count <- data.frame(matrix(ncol = length(x$feat), nrow = 0))
  count[1,] <- 0
  colnames(count) <- x$feat
  
  for (i in correlated_coloumns$F1) {
    count[1,which(colnames(count) == i)] <- count[1,which(colnames(count) == i)] + 1
  }
  for (i in correlated_coloumns$F2) {
    count[1,which(colnames(count) == i)] <- count[1,which(colnames(count) == i)] + 1
  }
  count <- as.data.frame(t(apply(count, 1, FUN=function(x) sort(x, decreasing=TRUE))))
  
  x <- c()
  k <- 1
  
  for ( i in 1:length(colnames(count))) {
    if(i < length(colnames(count))) {
      for (j in 1:length(correlated_coloumns$F1)) {
        if(colnames(count[i]) == correlated_coloumns$F1[j])
        {
          num <- which(colnames(count) == correlated_coloumns$F2[j])
          if((correlated_coloumns$F2[j] %in% x) == FALSE) {
            x[k] <- colnames(count[num])
            count <- select(count,-num)
            k <- k + 1 
          }
        }
      }
      for (j in 1:length(correlated_coloumns$F2)) {
        if(colnames(count[i]) == correlated_coloumns$F2[j])
        {
          num <- which(colnames(count) == correlated_coloumns$F1[j])
          if((correlated_coloumns$F1[j] %in% x) == FALSE) {
            x[k] <- colnames(count[num])
            count <- select(count,-num)
            k <- k + 1 
          }
        }
      }
    }
    else 
      break()
  }
  return(x)
}


```
## Removal of correlated columns

```{r}
###################################### No correlated columns ####################################

feature_remove <- correlations(correlationValue)
#dropping the columns
df_noCorr <- select(df_allF,-feature_remove)
print("There are no correlated columns")

#Data frame with reduced features "Scaled"
df_noCorr_scaled <- scale(df_noCorr)%>%data.frame()
```

## Elbow Method
Elbow method for getting optimal value of k. The idea behind elbow method is to to define number of clusters such that the total within-cluster sum of square (WSS) is minimized. The value of chosen k is such that adding another cluster does not affect the overall wss much.
```{r Elbow Method}
set.seed(123)
# Elbow method to get best k from k = 2 to k = 10.
k.max <- 10
rownames( df_noCorr_scaled ) <- seq_len( nrow( df_noCorr_scaled ) )
wss <- sapply(1:k.max, 
              function(k){kmeans(df_noCorr_scaled, k, nstart=50,iter.max = 15 )$tot.withinss})
wss

plot(1:k.max, wss,
     type="b", pch = 19, frame = FALSE, 
     xlab="Number of clusters K",
     ylab="Total within-clusters sum of squares")
title(main="Elbow Method")

```


## Bootstrap Sampling

Create different samples (with replacement) of data from the original dataset. With varying data (different samples), we can check if the clustering results are stable or not. So we are plotting a Box-plot for each value of K against "Total within sum of square errors (totwss)". We need to check if the result follows the same trend as the original elbow method does, or there is a significant change in the plot. 

```{r Bootstrap}
#No. of Bootsamples to be created
bootsamplesize <- seq(1:100)

max_k <-seq(1:10)
wss.boot <- data.frame()

set.seed(123)
for(i in bootsamplesize){

  #Create Index with replacement
  index <- sample(1:nrow(df_noCorr_scaled),nrow(df_noCorr_scaled),replace = T)

  #create samples from the index
  dfSample <-  df_noCorr_scaled[index,]

  #run for each value of k for the current sample
  for(k in max_k){

    kmclust <- kmeans(dfSample,k,nstart=50,iter.max = 25)

    #Store the data (1 row) of current K and withinss in the dataframe
    wss.boot <- rbind(wss.boot, c(k,kmclust$tot.withinss))

  }
}

colnames(wss.boot) <- c('k','totwss')


#Plot the Data
boxplot(totwss~k, data = wss.boot, xlab = "k values", ylab = "totwss", main = "Bootsampled Data: Kmeans Cluster withinss")


```


## Tuning for best hierarchical clustering method

There are different ways to calculate the distance/ similarity between individual points and in between the clusters which can be a very important factor in deciding the final cluster results.
So we first create a list of these methods and then create clusters for each combination.

Then we can calculate the clusters. The ac value of each cluster gives the quality/ tightness of each cluster. The closer the value is to 1 the better are the results.
```{r Hierarchical Tune}
tunehierarchical <- function() {
  set.seed(123)
  # methods to assess inter cluster similarity
  m <- c("average", "single", "complete", "ward")
  names(m) <- c("average", "single", "complete", "ward")
  
  #distance measures
  dist_methods <-
    c("euclidean", "manhattan", "maximum", "canberra", "minkowski")
  names(dist_methods) <-
    c("euclidean", "manhattan", "maximum", "canberra", "minkowski")
  
  # function to compute coefficient
  final_dist_m <- list()
  final_m <- list()
  for (dist_m in dist_methods)
  {
    dist_matrix <- dist(df_noCorr_scaled, method = dist_m)
    for (x in m)
    {
      hc <- agnes(dist_matrix, method = x)
      ac <- hc$ac
      title <- sprintf("Distance: %s,Inter cluster Distance: %s, ac value: %f",  dist_m, x, ac)
      pltree(hc, cex = 0.6, hang = -1, main = title)
      
      
      if (!is.nan(ac) && ac > 0.95)
      {
        print(title)
        final_dist_m <- c(final_dist_m, dist_m)
        final_m <- c(final_m, x)
      }
    }
  }
}
if(tuneHierarchical)
  tunehierarchical()
```
## Orclus Tuning

The following chunk tunes the hyper-parameters l (final cluster specific subspaces dimension) and k0 (initial number of clusters) by the “sparsity.coefficient” and the “within.projenss” measures obtained along with the clustering result where lower values for sparcity coefficient and wss are expected.
```{r OrclusTune, warning=FALSE}

#Represents the tuning of clusters according to quality measures

 tune_orclus <- function(num_of_clusters)
{
   tune_lf_sparsity<- data.frame(k = numeric(),dim = numeric(),sparsity = numeric(),proj = numeric())

   j<-1
  for (k in seq(10,30,by = 5)) {
  
    for (i in seq(20,60,by = 5)) {
  
      orclus_res_k2 <- orclus:: orclus(x = df_noCorr_scaled,k=2,l = i, k0 = k,verbose=FALSE)
      tune_lf_sparsity[j,1] <- k
      tune_lf_sparsity[j,2] <- i
      tune_lf_sparsity[j,3] <- orclus_res_k2$sparsity.coefficient
      tune_lf_sparsity[j,4] <- sum(orclus_res_k2$within.projens)
      j <- j+1
    }
  }
   tune_lf_sparsity$mult <- tune_lf_sparsity$sparsity * tune_lf_sparsity$proj
  opt <- tune_lf_sparsity[which(tune_lf_sparsity$mult == min(tune_lf_sparsity$mult)),]
  cat("Optimal value for sparsity is with k0 = ",opt$k,"average dimension = ",opt$dim,"gives sparcity coefficient = ",opt$sparsity,"and projected wss=",opt$within.projens)
   return (tune_lf_sparsity)
}
if(tuneOrclus)
  tune_orclus(numClustOrclus)

```

## Visualisation

Creating a summary data frame for capturing population mean.
```{r}
feature_summary_scaled<-data.frame(df_allF_scaled%>%
                                     summarise_each(mean))
feature_summary<-data.frame(df_allF%>%
                              summarise_each(mean))
```


For visualisation of clustering results we choose radial chart.
To generate this radial chart we need to format the data in a data structure suitable for plotting.
The bar height is the scaled value of the features of a cluster. The tooltip gives extra information about the actual feature value and the population mean for the feature.
```{r Visualisation}
#Creating a data frame which can be used for the visualization
options(scipen = 999)
createPlotData <- function(algo_name,
                           clus_name,
                           cluster_scaled,
                           cluster)
{
  cluster_plot_data <- 
    rbind(colnames(cluster[-1]), cluster_scaled[-1], feature_summary, cluster[-1]) 
  
  rownames(cluster_plot_data) <- 
    c( "feature", "scaled_cluster_feature_value", "population_mean", "cluster_feature_value")
  
  cluster_plot_data <-
    t(cluster_plot_data) %>% data.frame() %>% mutate(algorithm = algo_name, cluster_name = clus_name)
  return(cluster_plot_data)
}


getPlotData <- function(algo_name, final_labels, no_of_clusters)
{
  # labelling the records
  df_allF_labeled <- df_allF %>%
    mutate(label = final_labels)
  df_allF_scaled_labeled <- df_allF_scaled %>%
    mutate(label = final_labels)
  
  # creating cluster feature vectors from obtained labels
  
  cluster_features_scaled <- data.frame(df_allF_scaled_labeled %>%
                                          group_by(label) %>%
                                          summarise_each(mean))
  cluster_features <- data.frame(df_allF_labeled %>%
                                   group_by(label) %>%
                                   summarise_each(mean))
  plotData <- list()
  for (i in seq(1:no_of_clusters))
  {
    clus_name <- paste("Type ", i)
    plotData[[i]] <-
      createPlotData(algo_name, clus_name, cluster_features_scaled[i, ], cluster_features[i, ])
  }
  return(plotData)
  
}

#Needed because in Proclus we just show the selected dimensions

getProclusPlotData <- function(algo_name, proclus_cluster, cluster_num)
{
  # labelling the records
  df_noCorr_labeled <-
    df_noCorr[proclus_cluster[[cluster_num]]$objects, which(proclus_cluster[[cluster_num]][["subspace"]] == TRUE)] %>%
    mutate(label = cluster_num)
  df_noCorr_scaled_labeled <-
    df_noCorr_scaled[proclus_cluster[[cluster_num]]$objects, which(proclus_cluster[[cluster_num]][["subspace"]] == TRUE)] %>%
    mutate(label = cluster_num)
  
  # creating cluster feature vectors from obtained labels
  
  cluster_feature_scaled <- data.frame(df_noCorr_scaled_labeled %>%
                                         group_by(label) %>%
                                         summarise_each(mean))
  cluster_feature <- data.frame(df_noCorr_labeled %>%
                                  group_by(label) %>%
                                  summarise_each(mean))
  p_feature_summary <- data.frame(df_noCorr %>%
                                    summarise_each(mean))
  p_feature_summary <-
    p_feature_summary[, which(proclus_cluster[[cluster_num]][["subspace"]] == TRUE)]
  
  plotData <- list()
  
  clus_name <- paste("Type ", cluster_num)
  cluster_plot_data <-
    rbind( colnames(cluster_feature[-1]), cluster_feature_scaled[-1], p_feature_summary, cluster_feature[-1])
  rownames(cluster_plot_data) <-  c( "feature", "scaled_cluster_feature_value", "population_mean", "cluster_feature_value" )
  
  cluster_plot_data <-
    t(cluster_plot_data) %>% data.frame() %>% mutate(algorithm = algo_name, cluster_name = clus_name)
  
  return(cluster_plot_data)
}

```
## Decision Tree

Now we have all the feature values for each cluster. However, for each cluster there are only a few features which clearly distinguish it from other clusters. To find out which features and their respective values are responsible to identify a cluster we can use interpretable surrogate models.
Let us now use a Decision Tree algorithm to identify which features are the most important to lead us to get our cluster labels as leaf nodes.

```{r}
createDT<-function(final_labels,remove_na)
{ 
  set.seed(123)
  
  
  # labelling the records 
  df_allF_labeled<-df_allF%>%
    mutate(label = final_labels)
  df_string_labeled<-df_allF_labeled
  
  #Adding "Type" to each cluster number to make it non numeric
  df_string_labeled$label<-sub("^","Type ",df_allF_labeled$label)
  if(remove_na)
  {
    df_string_labeled <- df_string_labeled[-c(which(is.na(df_string_labeled$label))),]
  }
  
  trctrl <- trainControl(method = "boot", number = 10)
  dtree_fit <- train(label ~., data = df_string_labeled, method = "rpart",
                     parms = list(split = "information"),
                     trControl=trctrl,
                     tuneLength = 10)
  
  prp( dtree_fit$finalModel, extra = 2, box.palette = "Reds", tweak = 1.5, varlen = -10, branch.type = 5
  )
}
```
## Random Forest

We can use random forest for validating the decision tree obtained. This chunk  does the parameter tuning and gives the best mtry and ntree values. mtry is the number of variables randomly sampled as candidates at each split and ntree is the number of trees created by the function. We then apply random forest with the best paramters. 
The features are then ordered as per their mean decrease accuracy.
```{r RF}

##Tune random forest
tuneRandomForest <- function(labels, remove_na) {
  set.seed(123)
  # labelling the records
  data <- df_allF %>%
    mutate(label = labels)
  data$label <- as.factor(data$label)
  
  if (remove_na)
  {
    data <- data[-c(which(is.na(data$label))),]
  }
  x <- data[-ncol(data)]
  y <- data$label
  
  
  #tuning of rf
  customRF <- list(type = "Classification", library = "randomForest", loop = NULL)
  
  customRF$parameters <- data.frame( parameter = c("mtry", "ntree"),  class = rep("numeric", 2),label = c("mtry", "ntree")
    )
  
  customRF$grid <- function(x, y, len = NULL, search = "grid") {
  }
  
  customRF$fit <-function(x,y, wts, param, lev, last, weights, classProbs, ...) {
      randomForest(x, y, mtry = param$mtry, ntree = param$ntree, ...)
  }
  
  customRF$predict <- function(modelFit, newdata, preProc = NULL, submodels = NULL)
  predict(modelFit, newdata)
  
  customRF$prob <-  function(modelFit, newdata, preProc = NULL, submodels = NULL)
  predict(modelFit, newdata, type = "prob")
  
  
  # train model
  control <- trainControl(method = "repeatedcv",number = 5, repeats = 2)
  tunegrid <- expand.grid(.mtry = c(5:12), .ntree = c(500, 1000,1500))
  set.seed(123)
  custom <-  train( label ~ ., data = data, method = customRF, tuneGrid = tunegrid, trControl = control)
  
  plot(custom)
  print(custom)
  return(custom$bestTune)
}



##applying random forest
applyrf <- function(labels, numClusters, ntree, mtry, remove_na) {
  # labelling the records
  df_allF_labeled <- df_allF %>%
    mutate(label = labels)
  df_string_labeled <- df_allF_labeled
  df_string_labeled$label <- as.factor(df_string_labeled$label)

  if (remove_na)
  {
    df_string_labeled <- df_string_labeled[-c(which(is.na(df_string_labeled$label))), ]
  }
  
  data.rf <- randomForest(formula = label ~ .,data = df_string_labeled,importance = TRUE,ntree = ntree,mtry = mtry,
      proximity = TRUE)
  print(data.rf)
  plot(data.rf)
  varImpPlot(data.rf)  #the first graph shows how worse the model will perfrom after removing each variable and second shows how pure the nodes are at the end of the tree
  
  impfeat <- importance(data.rf)
  impfeatdf <- data.frame(impfeat)
  impfeatorder <- impfeatdf[order(-impfeatdf$MeanDecreaseAccuracy), ]
  impfeatorder[, "MeanDecreaseAccuracy", drop = FALSE]
}

```

## PCA-Kmeans
Prinicipal Component Analysis is used for High Dimensional Dataset to convert the possibly correlated features into linearly uncorrelated variables called Principal Components (PCs). The first Principal Component (PC) explains the most amount of variance in the data. The PCs are arranged in the decreasing order of variance explained. This can be seen in Scree Plots in the following chunks. To explain certain amount of variance of the data, we need to choose the subsequent amount of the PC number. After selecting first n PCs, we need to save that in another dataframe and then we can perform our clustering algorithm on the new transformed data.

The following chunk returns the number of principle components for a desired variance.
```{r}
getnumberPcs<-function(variance,km.pc){
  PCvar <- as.data.frame(t(summary(km.pc)$importance[3,]))
PC_num <- 0
for(i in seq(1:ncol(PCvar))){
    if(PCvar[1,i]<=variance)
    { 
      PC_num <- PC_num+1
    }
}
return(PC_num)
}

```
## Fetching labels for Performing Radial Chart Visualisation and Decision Tree for selected Cluster numbers

First the labels are fetched for each clustering algorithm, which is then passed into the initVis function alongwith the cluster number to create Radial Chart visualisations. Decision tree function is then called to visualize the decision points that can be used by the user to ask the relevant questions to come to a sub-group of Tinnitus.

# Global Clustering Methods

By global clustering we analyze the data as a whole. In other words, we consider all the dimensions in the data set as is without any form of dimension reductions.

# Subspace Clustering Methods

Subspace clustering tries to solve the problem of high dimensional data being nearly equidistant from each other by finding the clusters in different subspaces.

Proclus: Proclus can produce cluster with different number of selected dimensions as it tries to maintain the average dimensions per cluster given by the user. This algorithm accepts mainly two user specified hyper-parameters, namely K number of clusters and average dimensionality per cluster l.

Orclus: Many datasets contain attribute correlations. Orclus is used to tackle this by extending Proclus to include search for non-axis parallel subspaces. This is done by by building new dimensions which for a particular cluster using covariance matrix. This algorithm accepts mainly three hyper-parameters, namely final number of clusters k, final number of dimensions per cluster l and initial number of clusters k0.

```{r}

#----------------------------Global---------------------------------------#
getKmeanslabels<-function(numClust){
  set.seed(123)
  kmclust <- kmeans(df_noCorr_scaled, numClust)
  km_labels <- kmclust$cluster 
  return(list("labels"=km_labels,"numClust"=numClust))
}
   

gethKmeanslabels<-function(numClust,clustmethod){
  set.seed(123)
  hkmclust <- hkmeans(df_noCorr_scaled, numClust,hc.method = clustmethod)
  hkm_labels <- hkmclust$cluster 
  return(list("labels"=hkm_labels,"numClust"=numClust))
}
   
   
gethierarchicallabels<-function(numClust,clustmethod){
  set.seed(123)
  dist_matrix<-dist(df_noCorr_scaled, method = "euclidean")
  hc <-agnes(dist_matrix, method = clustmethod)
  h_labels<-cutree(hc,k=numClust)
  return(list("labels"=h_labels,"numClust"=numClust))
}
   
#----------------------------Subspace-------------------------------------#

getprocluslabels<-function(numClust,avgdim){
  set.seed(123)
  ProClus.clusters.k<- subspace:: ProClus(df_noCorr_scaled,k=numClust,d=avgdim)
  
  labels<-c()
  
  for ( i in 1:numClust) {
    for (j in ProClus.clusters.k[[i]]$objects) {
      labels[j] <- i
    }
  }
  return(list("labels"=labels,"numClust"=numClust,"objradial"=ProClus.clusters.k))
}
   
   
 getorcluslabels<-function(numClust,l,k0){
  set.seed(123)
  orclus_res_k <- orclus:: orclus(df_noCorr_scaled,k = numClust,l = l, k0 = k0,verbose=FALSE)
  ok_labels <- orclus_res_k$cluster
  return(list("labels"=ok_labels,"numClust"=numClust))
}
   
#----------------------------PCA---------------------------------------#
 
getpcakmeanslabels<-function(numClust,variance){
  set.seed(123)
  km.pc <- prcomp(df_noCorr_scaled, center = TRUE)
  
  #Maxmimum PC for Scree and Score Plots for visualization
  maxPC <- 50
  screeplot(km.pc, type = "l", npcs = maxPC, main = paste("Screeplot of the first ",
  maxPC," PCs"))
  #eigenvalues <1 would mean that the component actually explains less than a single
  #explanatory variable
  abline(h = 1, col="red", lty=5)
  legend("topright", legend=c("Eigenvalue = 1"),
  col=c("red"), lty=5, cex=0.6)
  
  PC_num <- getnumberPcs(variance,km.pc)
  cumpro <- cumsum(km.pc$sdev^2 / sum(km.pc$sdev^2))
  plot(cumpro[0:maxPC], xlab = "PC Numbers", ylab = "Amount of explained variance", main = paste("Scree plot of variance =", variance*100,"% at PC =",PC_num))
  abline(v = PC_num, col="blue", lty=5)
  abline(h = variance, col="blue", lty=5)
  legend("topleft", legend=c(paste("Cut-off @ PC",PC_num)), col=c("blue"), lty=5, cex=0.6)
  
  df_PC <- km.pc$x[,1:PC_num]
  pckmclust <- kmeans(df_PC, numClust)
  pckm_labels <- pckmclust$cluster 
  return(list("labels"=pckm_labels,"numClust"=numClust))
}
```

## Clustering implementation

These function calls are used to get the cluster assignment of the approches. User can store the Proclus result object to a file for future reuse as Proclus results are not reproducable.
```{r warning=FALSE} 

  kmeansres<-getKmeanslabels(numClustKmeans)

  hierarchicalres<-gethierarchicallabels(numClusthierarchical,clustering_method_hierarchical)
  
  hkmeansres<-gethKmeanslabels(numClusthKmeans,clustering_method_hkmeans)
  
  if(reproduceProclusResults){
      load(path)
    } else {
      proclusres<-getprocluslabels(numClustProclus,avgDimProclus)
    #saving the proclus results to view them later
    save(proclusres, file = paste("data",proclusres$numClust,".RData"))
    } 
  
  orclusres<-getorcluslabels(numClustOrclus,finalSubspaceDimOrclus,initialNumberOfClustersOrclus)
  
  pckmres<-getpcakmeanslabels(numClustPCAKmeans,variancePCAKmeans)
  print("Fetched the labels for all clustering methods")
  
```
## Output

The output in the form of decision trees and radial charts can be seen after running this chunk for number of clusters from 2 to 10.
```{r}
 message("Note:The radial charts can be seen only for 10 clusters")
#kmeans
   cluster_plot_data<-getPlotData("k-means",kmeansres$labels,kmeansres$numClust)
   r2d3(data = cluster_plot_data[[1]], script = "cluster_chart.js",viewer ="internal")
   r2d3(data = cluster_plot_data[[2]], script = "cluster_chart.js",viewer ="internal")

   if(kmeansres$numClust>2)
   r2d3(data = cluster_plot_data[[3]], script = "cluster_chart.js",viewer ="internal")
   if(kmeansres$numClust>3)
   r2d3(data = cluster_plot_data[[4]], script = "cluster_chart.js",viewer ="internal")
   if(kmeansres$numClust>4)
   r2d3(data = cluster_plot_data[[5]], script = "cluster_chart.js",viewer ="internal")
   if(kmeansres$numClust>5)
   r2d3(data = cluster_plot_data[[6]], script = "cluster_chart.js",viewer ="internal")
   if(kmeansres$numClust>6)
   r2d3(data = cluster_plot_data[[7]], script = "cluster_chart.js",viewer ="internal")
   if(kmeansres$numClust>7)
   r2d3(data = cluster_plot_data[[8]], script = "cluster_chart.js",viewer ="internal")
   if(kmeansres$numClust>8)
   r2d3(data = cluster_plot_data[[9]], script = "cluster_chart.js",viewer ="internal")
   if(kmeansres$numClust>9)
   r2d3(data = cluster_plot_data[[10]], script = "cluster_chart.js",viewer ="internal")
  createDT(kmeansres$labels,0)
  
  #hierarchical
  cluster_plot_data<-getPlotData("Hierarchical",hierarchicalres$labels,hierarchicalres$numClust)
   r2d3(data = cluster_plot_data[[1]], script = "cluster_chart.js",viewer ="internal")
   r2d3(data = cluster_plot_data[[2]], script = "cluster_chart.js",viewer ="internal")

   if(hierarchicalres$numClust>2)
   r2d3(data = cluster_plot_data[[3]], script = "cluster_chart.js",viewer ="internal")
   if(hierarchicalres$numClust>3)
   r2d3(data = cluster_plot_data[[4]], script = "cluster_chart.js",viewer ="internal")
   if(hierarchicalres$numClust>4)
   r2d3(data = cluster_plot_data[[5]], script = "cluster_chart.js",viewer ="internal")
   if(hierarchicalres$numClust>5)
   r2d3(data = cluster_plot_data[[6]], script = "cluster_chart.js",viewer ="internal")
   if(hierarchicalres$numClust>6)
   r2d3(data = cluster_plot_data[[7]], script = "cluster_chart.js",viewer ="internal")
   if(hierarchicalres$numClust>7)
   r2d3(data = cluster_plot_data[[8]], script = "cluster_chart.js",viewer ="internal")
   if(hierarchicalres$numClust>8)
   r2d3(data = cluster_plot_data[[9]], script = "cluster_chart.js",viewer ="internal")
   if(hierarchicalres$numClust>9)
   r2d3(data = cluster_plot_data[[10]], script = "cluster_chart.js",viewer ="internal")
  createDT(hierarchicalres$labels,0)
  
  #hkmeans
  cluster_plot_data<-getPlotData("Hk-means",hkmeansres$labels,hkmeansres$numClust)
   r2d3(data = cluster_plot_data[[1]], script = "cluster_chart.js",viewer ="internal")
   r2d3(data = cluster_plot_data[[2]], script = "cluster_chart.js",viewer ="internal")

   if(hkmeansres$numClust>2)
   r2d3(data = cluster_plot_data[[3]], script = "cluster_chart.js",viewer ="internal")
   if(hkmeansres$numClust>3)
   r2d3(data = cluster_plot_data[[4]], script = "cluster_chart.js",viewer ="internal")
   if(hkmeansres$numClust>4)
   r2d3(data = cluster_plot_data[[5]], script = "cluster_chart.js",viewer ="internal")
   if(hkmeansres$numClust>5)
   r2d3(data = cluster_plot_data[[6]], script = "cluster_chart.js",viewer ="internal")
   if(hkmeansres$numClust>6)
   r2d3(data = cluster_plot_data[[7]], script = "cluster_chart.js",viewer ="internal")
   if(hkmeansres$numClust>7)
   r2d3(data = cluster_plot_data[[8]], script = "cluster_chart.js",viewer ="internal")
   if(hkmeansres$numClust>8)
   r2d3(data = cluster_plot_data[[9]], script = "cluster_chart.js",viewer ="internal")
   if(hkmeansres$numClust>9)
   r2d3(data = cluster_plot_data[[10]], script = "cluster_chart.js",viewer ="internal")
  createDT(hkmeansres$labels,0)
  
  #Proclus
  
   r2d3(data = getProclusPlotData("Proclus",proclusres$objradial,1), script = "cluster_chart.js",viewer ="internal")
   r2d3(data = getProclusPlotData("Proclus",proclusres$objradial,2), script = "cluster_chart.js",viewer ="internal")
   
   if(proclusres$numClust>2)
   r2d3(data = getProclusPlotData("Proclus",proclusres$objradial,3), script = "cluster_chart.js",viewer ="internal")
   if(proclusres$numClust>3)
   r2d3(data = getProclusPlotData("Proclus",proclusres$objradial,4), script = "cluster_chart.js",viewer ="internal")
   if(proclusres$numClust>4)
   r2d3(data = getProclusPlotData("Proclus",proclusres$objradial,5), script = "cluster_chart.js",viewer ="internal")
   if(proclusres$numClust>5)
   r2d3(data = getProclusPlotData("Proclus",proclusres$objradial,6), script = "cluster_chart.js",viewer ="internal")
   if(proclusres$numClust>6)
   r2d3(data = getProclusPlotData("Proclus",proclusres$objradial,7), script = "cluster_chart.js",viewer ="internal")
   if(proclusres$numClust>7)
   r2d3(data = getProclusPlotData("Proclus",proclusres$objradial,8), script = "cluster_chart.js",viewer ="internal")
   if(proclusres$numClust>8)
   r2d3(data = getProclusPlotData("Proclus",proclusres$objradial,9), script = "cluster_chart.js",viewer ="internal")
   if(proclusres$numClust>9)
   r2d3(data = getProclusPlotData("Proclus",proclusres$objradial,10), script = "cluster_chart.js",viewer ="internal")
  createDT(proclusres$labels,1)
  
  #Orclus
  cluster_plot_data<-getPlotData("Orclus",orclusres$labels,orclusres$numClust)
   r2d3(data = cluster_plot_data[[1]], script = "cluster_chart.js",viewer ="internal")
   r2d3(data = cluster_plot_data[[2]], script = "cluster_chart.js",viewer ="internal")

   if(orclusres$numClust>2)
   r2d3(data = cluster_plot_data[[3]], script = "cluster_chart.js",viewer ="internal")
   if(orclusres$numClust>3)
   r2d3(data = cluster_plot_data[[4]], script = "cluster_chart.js",viewer ="internal")
   if(orclusres$numClust>4)
   r2d3(data = cluster_plot_data[[5]], script = "cluster_chart.js",viewer ="internal")
   if(orclusres$numClust>5)
   r2d3(data = cluster_plot_data[[6]], script = "cluster_chart.js",viewer ="internal")
   if(orclusres$numClust>6)
   r2d3(data = cluster_plot_data[[7]], script = "cluster_chart.js",viewer ="internal")
   if(orclusres$numClust>7)
   r2d3(data = cluster_plot_data[[8]], script = "cluster_chart.js",viewer ="internal")
   if(orclusres$numClust>8)
   r2d3(data = cluster_plot_data[[9]], script = "cluster_chart.js",viewer ="internal")
   if(orclusres$numClust>9)
   r2d3(data = cluster_plot_data[[10]], script = "cluster_chart.js",viewer ="internal")
  createDT(orclusres$labels,0)
  
  #pcakmeans
  cluster_plot_data<-getPlotData("PCA-Kmeans",pckmres$labels,pckmres$numClust)
   r2d3(data = cluster_plot_data[[1]], script = "cluster_chart.js",viewer ="internal")
   r2d3(data = cluster_plot_data[[2]], script = "cluster_chart.js",viewer ="internal")

   if(pckmres$numClust>2)
   r2d3(data = cluster_plot_data[[3]], script = "cluster_chart.js",viewer ="internal")
   if(pckmres$numClust>3)
   r2d3(data = cluster_plot_data[[4]], script = "cluster_chart.js",viewer ="internal")
   if(pckmres$numClust>4)
   r2d3(data = cluster_plot_data[[5]], script = "cluster_chart.js",viewer ="internal")
   if(pckmres$numClust>5)
   r2d3(data = cluster_plot_data[[6]], script = "cluster_chart.js",viewer ="internal")
   if(pckmres$numClust>6)
   r2d3(data = cluster_plot_data[[7]], script = "cluster_chart.js",viewer ="internal")
   if(pckmres$numClust>7)
   r2d3(data = cluster_plot_data[[8]], script = "cluster_chart.js",viewer ="internal")
   if(pckmres$numClust>8)
   r2d3(data = cluster_plot_data[[9]], script = "cluster_chart.js",viewer ="internal")
   if(pckmres$numClust>9)
   r2d3(data = cluster_plot_data[[10]], script = "cluster_chart.js",viewer ="internal")
  createDT(pckmres$labels,0)
```
## Validation of decision tree 
Note: As this chunk does the tuning of random forest, it may take some time. 

This chunk is used to display results of Random Forest which can be used for DT validation. The optimal parameters obtained after tuning are used for creating random forest for each method.

```{r RandomForest}

if(valDTkmeans){
 bestparamdf<-tuneRandomForest(kmeansres$labels,0)
 applyrf(kmeansres$labels,numClustKmeans,bestparamdf$ntree,bestparamdf$mtry,0)
}
if(valDThierarchical){
  bestparamdf<-tuneRandomForest(hierarchicalres$labels,0)
  applyrf(hierarchicalres$labels,numClusthierarchical,bestparamdf$ntree,bestparamdf$mtry,0)
}
if(valDThkmeans){
 bestparamdf<-tuneRandomForest(hkmeansres$labels,0)
 applyrf(hkmeansres$labels,numClusthKmeans,bestparamdf$ntree,bestparamdf$mtry,0)
}
if(valDTProclus){
   bestparamdf<-tuneRandomForest(proclusres$labels,1)
  applyrf(proclusres$labels,proclusres$numClust,bestparamdf$ntree,bestparamdf$mtry,1) 
}
if(valDTOrclus){
  bestparamdf<-tuneRandomForest(orclusres$labels,0)
  applyrf(orclusres$labels,numClustOrclus,bestparamdf$ntree,bestparamdf$mtry,0)
}
if(valDTPCAKmeans){
  bestparamdf<-tuneRandomForest(pckmres$labels,0)
  applyrf(pckmres$labels,numClustPCAKmeans,bestparamdf$ntree,bestparamdf$mtry,0)
}
 
 

```

## Evaluation:

To compare the clustering results obtained from k-means, agglomerative hierarchical clustering and Hk-means clustering  

* Dunn Index: Dunn Index calculates the ratio of the smallest distance between the points in different clusters to the largest distance between the points within the cluster. Higher value for Dunn Index is desirable. Dunn Index ranges from 0 to 1.

* Connectivity: The connectivity index measures the extent to which data points are placed in one cluster compared to its nearest neighbors. It ranges from zero to infinity. The lower the connectivity better are the results.

* Compactness or cluster cohesion: Compactness is calculated by silhouette width which is the average of silhouette value of all observations. This value ranges from 1 to -1, where values closer to 1 mean the clusters are compact and closer to -1 mean poorly formed clusters.

```{r Evaluation, echo=FALSE}
evaluate<-function(labels,numclusters,approach){
  
  result_set<-vector("list", 5)
  result_set[1]<-approach
  result_set[2]<-numclusters
  result_set[3]<-dunn(clusters = labels, Data=df_allF_scaled)
  result_set[4]<-connectivity(distance = NULL, labels, Data = df_allF_scaled, neighbSize = 10, method = "euclidean")
  result_set[5]<-diceR::compactness(df_allF_scaled, labels)
  return(result_set)
  
}

eval_table <- matrix(, nrow = 1, ncol = 5)
colnames(eval_table)<-c("Approach","Number of clusters","Dunn","Connectivity","Compactness")
eval_table<-rbind(eval_table,evaluate(kmeansres$labels,kmeansres$numClust,"kmeans"))
eval_table<-rbind(eval_table,evaluate(hierarchicalres$labels,hierarchicalres$numClust,"hierarchical"))
eval_table<-rbind(eval_table,evaluate(hkmeansres$labels,hkmeansres$numClust,"hkmeans"))
eval_table<-rbind(eval_table,evaluate(pckmres$labels,pckmres$numClust,"pca-kmeans"))
eval_table<-eval_table[-1,]
eval_table
```
